{"cells":[{"cell_type":"markdown","metadata":{"id":"DVUCruGZry4W"},"source":["# 1. Import and Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHqhiwBDry4Z","executionInfo":{"status":"ok","timestamp":1663359273644,"user_tz":-330,"elapsed":88938,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}},"outputId":"d4e48a7b-d1ba-454e-9c58-e6c53a905c35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.4.1\n","  Using cached tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n","Collecting tensorflow-gpu==2.4.1\n","  Downloading tensorflow_gpu-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3 MB)\n","\u001b[K     |████████████████████████████████| 394.3 MB 12 kB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n","Collecting mediapipe\n","  Downloading mediapipe-0.8.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n","\u001b[K     |████████████████████████████████| 31.5 MB 1.3 MB/s \n","\u001b[?25hCollecting sklearn\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.15.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.0)\n","Collecting h5py~=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 49.5 MB/s \n","\u001b[?25hCollecting typing-extensions~=3.7.4\n","  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n","Collecting absl-py~=0.10\n","  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 71.1 MB/s \n","\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.37.1)\n","Collecting wrapt~=1.12.1\n","  Downloading wrapt-1.12.1.tar.gz (27 kB)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.1.2)\n","Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (2.8.0)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (1.6.3)\n","Collecting gast==0.3.3\n","  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n","Collecting flatbuffers~=1.12.0\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Collecting numpy~=1.19.2\n","  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n","\u001b[K     |████████████████████████████████| 14.8 MB 60.1 MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.3.0)\n","Collecting grpcio~=1.32.0\n","  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 37.4 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (0.2.0)\n","Collecting tensorflow-estimator<2.5.0,>=2.4.0\n","  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 44.7 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.1) (3.17.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.0.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.2.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (22.1.0)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.6.0.66)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n","Building wheels for collected packages: wrapt, sklearn\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68720 sha256=eb99a398cf7ca96424a74bcf7944fc5123a2fdee9cf3312f7ad872be5fa7a51d\n","  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=93f941038fd1787105cb929077f2bf0d7b0c84fb77008e755ccf51998fa0fbbb\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","Successfully built wrapt sklearn\n","Installing collected packages: typing-extensions, numpy, grpcio, absl-py, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow-gpu, tensorflow, sklearn, mediapipe\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.1.1\n","    Uninstalling typing-extensions-4.1.1:\n","      Successfully uninstalled typing-extensions-4.1.1\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: grpcio\n","    Found existing installation: grpcio 1.48.1\n","    Uninstalling grpcio-1.48.1:\n","      Successfully uninstalled grpcio-1.48.1\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.2.0\n","    Uninstalling absl-py-1.2.0:\n","      Successfully uninstalled absl-py-1.2.0\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.14.1\n","    Uninstalling wrapt-1.14.1:\n","      Successfully uninstalled wrapt-1.14.1\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 2.0.7\n","    Uninstalling flatbuffers-2.0.7:\n","      Successfully uninstalled flatbuffers-2.0.7\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n","jax 0.3.17 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n","cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n","Successfully installed absl-py-0.15.0 flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 mediapipe-0.8.11 numpy-1.19.5 sklearn-0.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.1 typing-extensions-3.7.4.3 wrapt-1.12.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","typing_extensions"]}}},"metadata":{}}],"source":["!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python mediapipe sklearn matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"qYxvMjPUry4d","executionInfo":{"status":"error","timestamp":1663355427121,"user_tz":-330,"elapsed":39,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}},"outputId":"f9f0b6c3-effb-4fa0-906d-b9baba21f431"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-39be875d1360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmediapipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import cv2\n","import numpy as np\n","import os\n","from matplotlib import pyplot as plt\n","import time\n","import mediapipe as mp"]},{"cell_type":"markdown","metadata":{"id":"vzVxgJoOry4e"},"source":["# 2. Keypoints using MP Holistic"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9GNZS8Ary4f"},"outputs":[],"source":["mp_holistic = mp.solutions.holistic # Holistic model\n","mp_drawing = mp.solutions.drawing_utils # Drawing utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6e2qmFWary4g"},"outputs":[],"source":["def mediapipe_detection(image, model):\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n","    image.flags.writeable = False                  # Image is no longer writeable\n","    results = model.process(image)                 # Make prediction\n","    image.flags.writeable = True                   # Image is now writeable \n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n","    return image, results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ze061FLry4h"},"outputs":[],"source":["def draw_landmarks(image, results):\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7qNuvqbry4h"},"outputs":[],"source":["def draw_styled_landmarks(image, results):\n","    # Draw face connections\n","    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n","                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n","                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n","                             ) \n","    # Draw pose connections\n","    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n","                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n","                             ) \n","    # Draw left hand connections\n","    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n","                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n","                             ) \n","    # Draw right hand connections  \n","    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n","                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n","                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n","                             ) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wZU8i_0ry4j"},"outputs":[],"source":["cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    while cap.isOpened():\n","\n","        # Read feed\n","        ret, frame = cap.read()\n","\n","        # Make detections\n","        image, results = mediapipe_detection(frame, holistic)\n","        print(results)\n","        \n","        # Draw landmarks\n","        draw_styled_landmarks(image, results)\n","\n","        # Show to screen\n","        cv2.imshow('OpenCV Feed', image)\n","\n","        # Break gracefully\n","        if cv2.waitKey(10) & 0xFF == ord('q'):\n","            break\n","    cap.release()\n","    cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AzL5u8Oqry4l"},"outputs":[],"source":["len(results.left_hand_landmarks.landmark)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2N2iYQ5ry4m"},"outputs":[],"source":["results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4N4znMary4n"},"outputs":[],"source":["draw_landmarks(frame, results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r53FcP5Iry4n"},"outputs":[],"source":["plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"]},{"cell_type":"markdown","metadata":{"id":"JY69ommWry4o"},"source":["# 3. Extract Keypoint Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KJjk6fgry4o"},"outputs":[],"source":["len(results.left_hand_landmarks.landmark)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JP7zc8qcry4q"},"outputs":[],"source":["pose = []\n","for res in results.pose_landmarks.landmark:\n","    test = np.array([res.x, res.y, res.z, res.visibility])\n","    pose.append(test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWXLs6Qqry4q"},"outputs":[],"source":["pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n","face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n","lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bN5R-Exwry4q"},"outputs":[],"source":["face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() \n","    if results.face_landmarks \n","    else np.zeros(1404)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfQf2He7ry4r"},"outputs":[],"source":["def extract_keypoints(results):\n","    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n","    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n","    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n","    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n","    return np.concatenate([pose, face, lh, rh])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tZtozJ9ry4s"},"outputs":[],"source":["result_test = extract_keypoints(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jg1R3Mgry4s"},"outputs":[],"source":["result_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZgw8d3bry4t"},"outputs":[],"source":["468*3+33*4+21*3+21*3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWfN1YjYry4t"},"outputs":[],"source":["np.save('0', result_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5n2Vp4prry4u"},"outputs":[],"source":["np.load('0.npy')"]},{"cell_type":"markdown","metadata":{"id":"JjCXplkTry4u"},"source":["# 4. Setup Folders for Collection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QE4pnIqKry4u"},"outputs":[],"source":["# Path for exported data, numpy arrays\n","DATA_PATH = os.path.join('MP_Data') \n","\n","# Actions that we try to detect\n","actions = np.array(['വ', 'ീ','ണ'])\n","\n","# Thirty videos worth of data\n","no_sequences = 30\n","\n","# Videos are going to be 30 frames in length\n","sequence_length = 30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKtCxVqKry4v"},"outputs":[],"source":["# hello\n","## 0\n","## 1\n","## 2\n","## ...\n","## 29\n","# thanks\n","\n","# I love you"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCS3AX9Lry4v"},"outputs":[],"source":["for action in actions: \n","    for sequence in range(no_sequences):\n","        try: \n","            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n","        except:\n","            pass"]},{"cell_type":"markdown","metadata":{"id":"c7qGIbHJry4w"},"source":["# 5. Collect Keypoint Values for Training and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6BiX-MVry4w"},"outputs":[],"source":["from PIL import ImageFont, ImageDraw, Image\n","text_size = 18\n","b,g,r,a = 0,0,255,0\n","fontpath = \"./Samyak Malayalam.ttf\"     \n","font = ImageFont.truetype(fontpath,size=text_size,encoding='utf-8')\n","# orginal code\n","cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    \n","    # NEW LOOP\n","    # Loop through actions\n","    for action in actions:\n","        # Loop through sequences aka videos\n","        for sequence in range(no_sequences):\n","            # Loop through video length aka sequence length\n","            for frame_num in range(sequence_length):\n","\n","                # Read feed\n","                ret, frame = cap.read()\n","\n","                # Make detections\n","                image, results = mediapipe_detection(frame, holistic)\n","#                 print(results)\n","\n","                # Draw landmarks\n","                draw_styled_landmarks(image, results)\n","                \n","                # NEW Apply wait logic\n","                if frame_num == 0: \n","                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n","                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n","                    cv2.putText(image, 'Collecting frames for    Video Number {}'.format(sequence), (15,12), \n","                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    img_pil = Image.fromarray(image)\n","                    draw = ImageDraw.Draw(img_pil)\n","                    draw.text((200,-5), action, font = font, fill = (b, g, r, a))\n","                    image = np.array(img_pil)\n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","                    cv2.waitKey(2000)\n","                else: \n","                    cv2.putText(image, 'Collecting frames for     Video Number {}'.format(sequence), (15,12), \n","                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n","                    img_pil = Image.fromarray(image)\n","                    draw = ImageDraw.Draw(img_pil)\n","                    draw.text((200,-5), action, font = font, fill = (b, g, r, a))\n","                    image = np.array(img_pil)\n","                \n","                    # Show to screen\n","                    cv2.imshow('OpenCV Feed', image)\n","                \n","                # NEW Export keypoints\n","                keypoints = extract_keypoints(results)\n","                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n","                np.save(npy_path, keypoints)\n","\n","                # Break gracefully\n","                if cv2.waitKey(10) & 0xFF == ord('q'):\n","                    break\n","                    \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpowAwjKry4x"},"outputs":[],"source":["cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"K5DE_ybzry4x"},"source":["# 6. Preprocess Data and Create Labels and Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1qcoE_cry4y"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTvE6NO-ry4y"},"outputs":[],"source":["label_map = {label:num for num, label in enumerate(actions)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37Mq75Q9ry4z"},"outputs":[],"source":["label_map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sncL7S--ry4z"},"outputs":[],"source":["sequences, labels = [], []\n","for action in actions:\n","    for sequence in range(no_sequences):\n","        window = []\n","        for frame_num in range(sequence_length):\n","            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n","            window.append(res)\n","        sequences.append(window)\n","        labels.append(label_map[action])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2O0spMTry4z"},"outputs":[],"source":["np.array(sequences).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clGx7SW5ry40"},"outputs":[],"source":["np.array(labels).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmdIEl3rry40"},"outputs":[],"source":["X = np.array(sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UmWPtKQ5ry40"},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mjCURBIry41"},"outputs":[],"source":["y = to_categorical(labels).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvQC1FTHry41"},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SavcAfMry41"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ak-rMjWlry42"},"outputs":[],"source":["y_test.shape"]},{"cell_type":"markdown","metadata":{"id":"OaAsmLKMry42"},"source":["# 7. Build and Train LSTM Neural Network"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BFEYp8mZry42","executionInfo":{"status":"ok","timestamp":1664564450326,"user_tz":-330,"elapsed":3762,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}}},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.keras.callbacks import TensorBoard"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"lziKSBiZry43","executionInfo":{"status":"error","timestamp":1664564450691,"user_tz":-330,"elapsed":380,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}},"outputId":"b413d803-a74b-48bc-88ad-c69b9560813d","colab":{"base_uri":"https://localhost:8080/","height":182}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5303dfcf67b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtb_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}],"source":["log_dir = os.path.join('Logs')\n","tb_callback = TensorBoard(log_dir=log_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdcA0Uaqry43","executionInfo":{"status":"aborted","timestamp":1664564450692,"user_tz":-330,"elapsed":10,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}}},"outputs":[],"source":["model = Sequential()\n","model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n","model.add(LSTM(128, return_sequences=True, activation='relu'))\n","model.add(LSTM(64, return_sequences=False, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(actions.shape[0], activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybZcg5oCry44","executionInfo":{"status":"aborted","timestamp":1664564450693,"user_tz":-330,"elapsed":10,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}}},"outputs":[],"source":["res = [.7, 0.2, 0.1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHL_HDvSry44","executionInfo":{"status":"aborted","timestamp":1664564450694,"user_tz":-330,"elapsed":11,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}}},"outputs":[],"source":["actions[np.argmax(res)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KI0vxGo9ry45","executionInfo":{"status":"aborted","timestamp":1664564450695,"user_tz":-330,"elapsed":11,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}}},"outputs":[],"source":["model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"XSIvjuiGry45","executionInfo":{"status":"aborted","timestamp":1664564450695,"user_tz":-330,"elapsed":11,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}}},"outputs":[],"source":["model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kr7ji8Hyry45","executionInfo":{"status":"aborted","timestamp":1664564450696,"user_tz":-330,"elapsed":12,"user":{"displayName":"Syla Bavakutty","userId":"10383535632421728674"}}},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"Q5z4EiiDry46"},"source":["# 8. Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPx0F4Cmry46"},"outputs":[],"source":["res = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xl49seH-ry46"},"outputs":[],"source":["actions[np.argmax(res[0])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vm-ZiilZry47"},"outputs":[],"source":["actions[np.argmax(y_test[0])]"]},{"cell_type":"markdown","metadata":{"id":"L6W4oI7qry47"},"source":["# 9. Save Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IMLhM8-ry47"},"outputs":[],"source":["model.save('action.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiQDknRCry48"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iFwX3T1ry48"},"outputs":[],"source":["model.load_weights('action.h5')"]},{"cell_type":"markdown","metadata":{"id":"3RgcGmPRry48"},"source":["# 10. Evaluation using Confusion Matrix and Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypyAT1oYry49"},"outputs":[],"source":["from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaXSizscry49"},"outputs":[],"source":["yhat = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AufEd03jry49"},"outputs":[],"source":["ytrue = np.argmax(y_test, axis=1).tolist()\n","yhat = np.argmax(yhat, axis=1).tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slAD4QcYry4-"},"outputs":[],"source":["multilabel_confusion_matrix(ytrue, yhat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMCWd3hEry4-"},"outputs":[],"source":["accuracy_score(ytrue, yhat)"]},{"cell_type":"markdown","metadata":{"id":"F__5ER-Kry4-"},"source":["# 11. Test in Real Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qhY1GsYry4-"},"outputs":[],"source":["colors = [(245,117,16), (117,245,16), (16,117,245)]\n","def prob_viz(res, actions, input_frame, colors):\n","    output_frame = input_frame.copy()\n","    for num, prob in enumerate(res):\n","        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n","        #cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n","        img_pil = Image.fromarray(output_frame)\n","        draw = ImageDraw.Draw(img_pil)\n","        draw.text((20,70), actions[0], font = font, fill = (b, g, r, a))\n","        draw.text((20,100), actions[1], font = font, fill = (b, g, r, a))\n","        draw.text((20,150), actions[2], font = font, fill = (b, g, r, a))\n","        output_frame = np.array(img_pil)\n","        \n","    return output_frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VXu3Qceqry4_"},"outputs":[],"source":["draw.text??"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHc-Uga1ry4_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FwJV5U1ry4_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaIYsn5Ory4_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkXA2Sg6ry5A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"2lBV8upjry5A"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQXmWVVsry5A"},"outputs":[],"source":["# 1. New detection variables\n","from PIL import ImageFont, ImageDraw, Image\n","text_size = 18\n","b,g,r,a = 255,255,255,0\n","fontpath = \"./Samyak Malayalam.ttf\"     \n","font = ImageFont.truetype(fontpath,size=text_size,encoding='utf-8')\n","\n","\n","\n","sequence = []\n","sentence = []\n","threshold = 0.8\n","\n","cap = cv2.VideoCapture(0)\n","# Set mediapipe model \n","with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","    while cap.isOpened():\n","\n","        # Read feed\n","        ret, frame = cap.read()\n","\n","        # Make detections\n","        image, results = mediapipe_detection(frame, holistic)\n","        print(results)\n","        \n","        # Draw landmarks\n","        draw_styled_landmarks(image, results)\n","        \n","        # 2. Prediction logic\n","        keypoints = extract_keypoints(results)\n","#         sequence.insert(0,keypoints)\n","#         sequence = sequence[:30]\n","        sequence.append(keypoints)\n","        sequence = sequence[-30:]\n","        \n","        if len(sequence) == 30:\n","            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n","            print(actions[np.argmax(res)])\n","            \n","            \n","            \n","        #3. Viz logic\n","            if res[np.argmax(res)] > threshold: \n","                if len(sentence) > 0: \n","                    if actions[np.argmax(res)] != sentence[-1]:\n","                        sentence.append(actions[np.argmax(res)])\n","                else:\n","                    sentence.append(actions[np.argmax(res)])\n","\n","            if len(sentence) > 5: \n","                sentence = sentence[-5:]\n","\n","            # Viz probabilities\n","            image = prob_viz(res, actions, image, colors)\n","            \n","        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n","        #cv2.putText(image, ' '.join(sentence), (3,30), \n","                       #cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n","        img_pil = Image.fromarray(image)\n","        draw = ImageDraw.Draw(img_pil)\n","        draw.text((100,-3), ' '.join(sentence) ,align=\"left\", font = font, fill = (b, g, r, a))\n","        image = np.array(img_pil)\n","        # Show to screen\n","        cv2.imshow('OpenCV Feed', image)\n","\n","        # Break gracefully\n","        if cv2.waitKey(10) & 0xFF == ord('q'):\n","            break\n","    cap.release()\n","    cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbWWmqpNry5B"},"outputs":[],"source":["cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkPORdpQry5B"},"outputs":[],"source":["res[np.argmax(res)] > threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQPJhQ1Qry5B"},"outputs":[],"source":["(num_sequences,30,1662)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGsfm8GWry5B"},"outputs":[],"source":["model.predict(np.expand_dims(X_test[0], axis=0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdJUwOJ1ry5C"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}